## **Detecção de Fake News utilizando Naive Bayes**

### Bibliotecas

import pandas as pd
import numpy as np
import seaborn as sns
from google.colab import files
import io

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.util import ngrams
from nltk.tokenize import word_tokenize, sent_tokenize
import re
import string

from sklearn.model_selection import train_test_split

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.naive_bayes import MultinomialNB

from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

### Coleta de Dados e Análise Exploratória

uploaded = files.upload()

# Dataset do Kaggle https://www.kaggle.com/hassanamin/textdb3

df = pd.read_csv(io.BytesIO(uploaded['fake_or_real_news.csv']))

print(df.shape)
print('\n')
print(df.info)
print('\n')
df.head()

df.describe()

df.isnull().sum()

sns.countplot(df.label)

### Pré-processamento



df.columns = ['ID#', 'Title', 'Text', 'Label']

df = df.replace('FAKE', 0)
df = df.replace('REAL',1)

df['Text'] = df['Title'] + ' ' + df['Text']
df.head()

news = df.drop(['ID#','Title'], axis=1)
news.head()

# Letras minusculas, remover [], links, palavras que tenham numeros e pontuações
def clean_text(text):
  text = str(text).lower()
  text = re.sub('\[.*?\]', '', text)
  text = re.sub('https?;//\S+|www\,\,\S+', '', text)
  text = re.sub('<.*?>+', '', text)
  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
  text = re.sub('\n', '', text)
  text = re.sub('\w*\d\w*', '', text)
  return text
news['Text'] = news['Text'].apply(lambda x:clean_text(x))

news.head()

# Remover stop words

stop = stopwords.words('english')
news['Text'] = news['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

news.head()

# Lemmatize (Deixar apenas a raiz das palavras)

def lemmatize_words(text):
  wnl = nltk.stem.WordNetLemmatizer()
  lem = ' '.join([wnl.lemmatize(word) for word in text.split()])
  return lem

news['Text'] = news['Text'].apply(lemmatize_words)

news.head()

### Mineração de Dados

# Dividir o Dataset em Treino e Teste

X_train, X_test, y_train, y_test = train_test_split(news['Text'],
                                                    news['Label'], test_size=0.25)
display(X_train.head())
print('\n')
display(y_train.head())

print("\nThere are {} documents in the training data.".format(len(X_train)))


#Extração de Features

my_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)

tfidf_train = my_tfidf.fit_transform(X_train)
tfidf_test = my_tfidf.transform(X_test)

tfidf_train

#Aplicação do Naive Bayes

nb_classifier = MultinomialNB()
nb_classifier.fit(tfidf_train, y_train)

pred1 = nb_classifier.predict(tfidf_test)

### Pós-processamento

#Métricas do Algoritmo

print(classification_report(y_test, pred1, target_names= ['Fake', 'True']))

acc_score = accuracy_score(y_test, pred1)

conf_mat = confusion_matrix(y_test, pred1)

print(acc_score)
print('\n')
print(conf_mat)

df1 = pd.DataFrame(X_test)
df1['Valor_Real'] = pd.DataFrame(y_test)
df1['Valor_Previsto'] = pred1

display(df1.head())

Falso_Negativo = df1[(df1.Valor_Real == 0) & (df1.Valor_Previsto == 1)]
Falso_Positivo = df1[(df1.Valor_Real == 1) & (df1.Valor_Previsto == 0)]

display(Falso_Negativo.head())
print('\n')
display(Falso_Positivo.head())

--> DETECÇÃO DE FAKE NEWS UTILIZANDO O ALGORITMO NAIVE BAYES.
poƌ
Rodrigo Cardoso Durgieǁicz 
